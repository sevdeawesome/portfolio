<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SafetyBot</title>
    <link rel="stylesheet" href="index.css">
    <!-- bootstrap -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Gabarito:wght@400;900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

</head>


<body>
    <!-- Header with Subheader -->
    <!-- <section class="py-5">
        <h1>AGI<span style="color:#8c0010">Risk</span>.org</h1>
        <p>An AI trained to convince you of the gravity of the existential threat posed by AI<br>
        Inspired by <a target="_blank" href="https://arxiv.org/abs/2303.03885">AI Risk Skepticism</a> by Dr. Roman Yampolskiy</p>
    </section> -->


    <section class="py-5">
        <div class="container">
            <div class="row">
                <!-- Title -->
                <div class="col-md-6">
                    <h3>
                        AGI Risk
                    </h3>
                </div>
                
                <!-- Paragraph -->
                <div class="col-md-6 " >
                    <div class="right">
                         <a href="#chatbot" class="btn top-buttons">Chatbot</a>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <div class="hero d-flex align-items-center justify-content-center vh-75">
        <div class="container">
            <div class="content">

                <div class="row">
                    <!-- <div class="col-md-2"></div> -->
                    <div class="col-md-12">
                        <h1>Experimental chatbot created to explain the dangers of AI</h1>
                        <h2 id="herosubtext">My attempt at a solution to AI Risk Skepticism, inspired by Dr. Roman Yampolskiy's <a target="_blank" id="yampolskiylink" href="https://arxiv.org/abs/2303.03885">AI Risk Skepticism</a></h2>
                        <a href="#chatbot" class="btn btn-below"><h3>Chatbot</h3></a>
                    </div>
                   
                </div>
                
            </div>
        </div>
    </div>

    <!-- create a button to go to the chatbot -->
    <!-- <section class="container">
        <a href="https://sevdeawesome-safetybot.hf.space?__theme=dark" class="btn btn-danger btn-lg btn-block">Chat with SafetyBot > </a>
    </section> -->



    <!-- Chatbot -->
    <!-- <section class="container">
            <iframe src="https://sevdeawesome-safetybot.hf.space?__theme=dark" frameborder="0" width="100%" height="900px"></iframe>
    </section> -->
    
    

    <br><br><br><br>
    <div class="container" id="chatbot">
    <center>
    <h2 class="header">Chatbot</h1>

    </center>
            <iframe src="https://sevdeawesome-safetybot.hf.space?__theme=dark" frameborder="0" width="100%" height="900px"></iframe>

    </div>
    <br><br>
    <div class="container">

        <h2>Taxonomy of Objections to AI Safety</h2>

        <!-- Section 1: Priority Objections -->
        <div>
            <a href="#" data-toggle="collapse" data-target="#argument1" aria-expanded="false">
                Priority Objections
            </a>
            <div class="collapse" id="argument1">
                <p>1.1 Priority objection: AGI is Too Far so it isn't worth worrying about</p>
                <p>1.2 Priority objection: A Soft Takeoff is more likely and so we will have Time to Prepare</p>
                <p>1.3 Priority objection: There is No Obvious Path to Get to AGI from Current AI</p>
                <p>1.4 Priority objection: Something Else is More Important than AI safety / alignment</p>
                <p>1.5 Priority objection: Short Term AI Concerns are more important than AI safety</p>

                <br>
                <p>
                    Arguments in this category typically grant the risk proposition, but think there are other priorities more important, agi is too far, etc. Thoughts:
                    <ul>
                        <li>We will only ever be too early or too late to safeguard exponential technologies.</li>
                        <li>It is either too soon or too late to start worrying. Isn’t prudence preferable? </li>
                        <li>It is clear: controlled ASI is more difficult to achieve than ASI.</li>
                        <li>AI is entangled in other issues: global warming, pandemics, etc. Intelligence has the ability to make these problems better or worse.</li>
                        <li>No one knows the speed of takeoff</li>
                    </ul>
                    
                    
                    

                </p>
            </div>
        </div>

        <!-- Section 2: Technical Objections to AI Safety -->
        <div class="mt-2">
            <a href="#" data-toggle="collapse" data-target="#argument2" aria-expanded="false">
                Technical Objections to AI Safety
            </a>
            <div class="collapse" id="argument2">
                <p>2.1 Technical Objection: AI / AGI Doesn’t Exist, developments in AI are not necessarily progress towards AGI</p>
                <p>2.2 Technical Objection: Superintelligence is Impossible</p>
                <p>2.3 Technical Objection: Self-Improvement is Impossible</p>
                <p>2.4 Technical Objection: AI Can’t be Conscious Proponents argue that in order to be dangerous AI has to be conscious</p>
                <p>2.5 Technical Objection: AI Can just be a Tool</p>
                <p>2.6 Technical Objection: We can Always just turn it off</p>
                <p>2.7 Technical Objection: We can reprogram AIs if we don't like what they do</p>
                <p>2.8 Technical Objection: AI Doesn't have a body so it can't hurt us</p>
                <p>2.9 Technical Objection: If AI is as Capable as You Say, it Will not Make Dumb Mistakes</p>
                <p>2.10 Technical Objection: Superintelligence Would (Probably) Not Be Catastrophic</p>
                <p>2.11 Technical Objection: Self-preservation and Control Drives Don't Just Appear They Have to be Programmed In</p>
                <p>2.12 Technical Objection: AI can't generate novel plans</p>

                <br>
                <p>(From Kaj Sotola - Disjunctive Scenarios of AI Risk) Core arguments for AI safety can often be reduced to:
                    <ol>
                        <li>The capability claim: AI can become capable enough to potentially inflict major damage to human well-being.</li>
                        <li>The value claim: AI may act according to values which are not aligned with those of humanity, and in doing so cause considerable harm</li>
                    </ol>
                    (From Bostrom - Superintelligence) Bostroms argument goes as:
                    <ol>
                        <li>An AGI could become superintelligent</li>
                        <li>Superintelligence would enable the AGI to take over the world</li>
                    </ol>

                    I believe technical objections can usually reduce to an objection to one of these.
                    <ul>

                        <li>On 2.1: OpenAI, Antropic, et al have an explicit goal of general intelligence. </li>
                        <li>On 2.2, 2.3: This argument presumes a hard limit to intelligence. Of course, so long as the limit is above humans, this is irrelevant</li>
                        <li>On 2.4: AI Risk is not predicated on AI systems experiencing qualia. See Alan Turing “Argument from Consciousness” in his seminal paper</li>
                        <li>On 2.6, 2.7, 2.8, 2.9, 2.10, 2.11: see <a href="https://pauseai.info/xrisk" id="yampolskiylink">instrumental convergence</a> from pauseai. You cannot bring the coffee if you are turned off</li>
                        <li>Modern viruses are a subset of self reproducing AI’s. Given the challenges of deactivating them it is evident why turning off AI is not straightforward.</li>
                    </ul>
                    

                    

                    

                   

                    

                    
                    
                    </p>
            </div>
        </div>

        <!-- Section 3: AI Safety Objections -->
        <div class="mt-2">
            <a href="#" data-toggle="collapse" data-target="#argument3" aria-expanded="false">
                AI Safety Objections
            </a>
            <div class="collapse" id="argument3">
                <p>3.1 AI Safety Objections: AI Safety Can’t be Done Today</p>
                <p>3.2 AI Safety Objections: AI Can’t be Safe</p>
                <p>
                    <ul><li>There are two known options: prudence or negligence.</li>
                        <li>On 3.1: There are many papers that disprove this. Yampolskiy, R.V., Artificial superintelligence: a futuristic approach. 2015: cRc Press.</li>
                        <li>On 3.2: The first step to failure is not trying</li>
                     </ul>

                    





                </p>
                
            </div>
        </div>

        <!-- Section 4: Ethical Objections -->
        <div class="mt-2">
            <a href="#" data-toggle="collapse" data-target="#argument4" aria-expanded="false">
                Ethical Objections
            </a>
            <div class="collapse" id="argument4">
                <p>4.1 Ethical Objections: Superintelligence is Benevolence</p>
                <p>4.2 Ethical Objections: Let the Smarter Beings Win</p>
                <p>
                    <ul>
                        <li>On 4.1: See Bostroms Orthogonality Thesis
                            Armstrong, S., General purpose intelligence: arguing the orthogonality thesis. Analysis and
                            Metaphysics, 2013(12): p. 68-84.</li>
                            <li>On 4.2: The vast majority of humanity is not on board with self destruction</li>
                    </ul>

                    

                </p>
            </div>
        </div>

        <!-- Section 5: Biased Objections -->
        <div class="mt-2">
            <a href="#" data-toggle="collapse" data-target="#argument5" aria-expanded="false">
                Biased Objections
            </a>
            <div class="collapse" id="argument5">
                <p>5.1 Biased Objections: AI Safety Researchers are Non-Coders</p>
                <p>5.2 Biased Objections: Majority of AI Researchers is not Worried</p>
                <p>5.3 Biased Objections: Keep it Quiet</p>
                <p>5.4 Biased Objections: Safety Work just Creates an Overhead Slowing Down Research</p>
                <p>5.5 Biased Objections: Heads in the Sand</p>
                <p>5.6 Biased Objections: If we don't do it, Someone else will</p>
                <p>5.7 Biased Objections: AI Safety Requires Global Cooperation</p>
                <p>

                    <ul>
                        <li>On 5.1: Per Yampolskiy one doesn’t need to write code in order to understand the inherent risk of AGI, just like someone doesn’t have to work in a wet lab to understand dangers of pandemics from biological weapons.</li>
                        <li>On 5.2: Per Yampolskiy: Not only is this untrue, it is also irrelevant, even if 100% of mathematicians believed 2 + 2 = 5, it would still be wrong. Scientific facts are not determined by democratic process, and you don’t get to vote on reality or truth.</li>
                        <li>On 5.7: Catastrophic risks are present in more than one country. Global cooperation is required to address them. AI Safety experts generally grant this.</li>
                    </ul>


                    




                </p>
            </div>

        </div>

        <!-- Section 6: Miscellaneous Objections -->
        <div class="mt-2">
            <a href="#" data-toggle="collapse" data-target="#argument6" aria-expanded="false">
                Miscellaneous Objections
            </a>
            <div class="collapse" id="argument6">
                <p>6.1 Miscellaneous Objection: So Easy it will be Solved Automatically</p>
                <p>6.2 Miscellaneous Objection: AI Regulation Will Prevent Problems</p>
            </div>
        </div>

    </div>
   
    
<br><br><br>

    <section class="text-center py-5 container-fluid">
        <div class="row">
            <!-- Image Column -->
            <div class="col-12 col-lg-4 d-flex justify-content-center justify-content-lg-end">
                <img src="Stephen-Hawking-387288.jpg" style="max-height:120px;" alt="Stephen Hawking" class="img-fluid rounded-circle">
            </div>
            
            <!-- Quote Column -->
            <div class="col-12 col-lg-8 d-flex align-items-center">
                <blockquote class="blockquote">
                    <p>The development of full artificial intelligence could spell the <span style="color:#8c0010">end of the human race.</span></p>
                    <footer class="blockquote-footer">Stephen Hawking</footer>
                </blockquote>
            </div>
        </div>
    </section>
    <section class="text-center py-5 container-fluid">
        <div class="row">
            <!-- Image Column -->
            <div class="col-12 col-lg-4 d-flex justify-content-center justify-content-lg-end">
                <img src="sam_altman.jpg" style="max-height:120px;" alt="Stephen Hawking" class="img-fluid rounded-circle">
            </div>
            
            <!-- Quote Column -->
            <div class="col-12 col-lg-8 d-flex align-items-center">
                <blockquote class="blockquote">
                    <p>Development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity. </p>
                    <footer class="blockquote-footer">Sam Altman, OpenAI CEO - <a style="color:#8c0010 !important" href="https://blog.samaltman.com/machine-intelligence-part-1">link</a></footer>
                </blockquote>
            </div>
        </div>
    </section>



    <div class="container">

        <blockquote class="blockquote">
            <p>AI could view us as a threat</p>
            <footer class="blockquote-footer">Bill Gates <br><a style="color:#8c0010 !important" href="https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/">link</a></footer>
            <br>
            <p>This is an existential threat</p>
            <footer class="blockquote-footer">Geoffrey Hinton, Turing Award Winner, widely considered the "Godfather of AI" <br><a style="color:#8c0010 !important" href="https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/">link</a></footer>
            <br>
            <p>AI Has the potential to destroy civilization </p>
            <footer class="blockquote-footer">Elon Musk <br><a style="color:#8c0010 !important" href="https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html">link</a></footer>
            <br>
            <p>An open letter calling to <a id="yampolskiylink" href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">pause giant ai experiments more powerful than gpt-4</a> has been signed by many researchers steeped in the field</p>
        </blockquote>

    </div>
<br><br>
   
    <div class="container">
        <h2>Resources:</h2>
        <p>
        Visit <a href="https://pauseai.info/" id="yampolskiylink">pauseai.info</a> and <a href="https://futureoflife.org/" id="yampolskiylink">futureoflife.org</a> to contribute to AI Safety and AI Governance<br>
        <br>
        </p>
    </div>

    <br><br>

   
   

    <!-- Footer -->
    <footer class="py-3" id="bottomfooter">
        <center>Design is a WIP</center>        

    </footer>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.2/dist/js/bootstrap.bundle.min.js"></script>

</body>

</html>